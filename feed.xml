<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://utksi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://utksi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-15T01:05:05+00:00</updated><id>https://utksi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://utksi.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://utksi.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://utksi.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Cooperative Graph Neural Networks</title><link href="https://utksi.github.io/blog/2024/cognn/" rel="alternate" type="text/html" title="Cooperative Graph Neural Networks"/><published>2024-05-01T07:18:00+00:00</published><updated>2024-05-01T07:18:00+00:00</updated><id>https://utksi.github.io/blog/2024/cognn</id><content type="html" xml:base="https://utksi.github.io/blog/2024/cognn/"><![CDATA[<h2 id="detailed-analysis-of-cooperative-graph-neural-networks-co-gnns">Detailed Analysis of Cooperative Graph Neural Networks (Co-GNNs)</h2> <ul> <li>Proposed by <a href="https://arxiv.org/abs/2310.01267">Finkelstein et al.</a></li> </ul> <h3 id="framework-overview"><strong>Framework Overview</strong></h3> <p>Co-GNNs introduce a novel, flexible message-passing mechanism where each node in the graph dynamically selects from the actions: <code class="language-plaintext highlighter-rouge">listen</code>, <code class="language-plaintext highlighter-rouge">broadcast</code>, <code class="language-plaintext highlighter-rouge">listen and broadcast</code>, or <code class="language-plaintext highlighter-rouge">isolate</code>. This is facilitated by two cooperating networks:</p> <ol> <li><strong>Action Network ($\large{\pi}$)</strong>: Determines the optimal action for each node.</li> <li><strong>Environment Network ($\large{\eta}$)</strong>: Updates the node states based on the chosen actions.</li> </ol> <h2 id="mathematical-formulation"><strong>Mathematical Formulation</strong></h2> <ol> <li><strong>Action Selection (Action Network π)</strong>: <ul> <li>For each node $\large{v}$ , the action network predicts a probability distribution $\large{p^{(\ell)}_v}$ over the actions {S, L, B, I} at layer $\ell$ .</li> </ul> \[p^{(\ell)}_v = \pi \left( h^{(\ell)}_v, \{ h^{(\ell)}_u \mid u \in N_v \} \right)\] <ul> <li>Actions are sampled using the Straight-through Gumbel-softmax estimator.</li> </ul> </li> <li><strong>State Update (Environment Network η)</strong>: <ul> <li>The environment network updates the node states based on the selected actions.</li> </ul> \[h^{(\ell+1)}_v = \begin{cases} \eta^{(\ell)} \left( h^{(\ell)}_v, \{ \} \right) &amp; \text{if } a^{(\ell)}_v = \text{I or B} \\ \eta^{(\ell)} \left( h^{(\ell)}_v, \{ h^{(\ell)}_u \mid u \in N_v, a^{(\ell)}_u = \text{S or B} \} \right) &amp; \text{if } a^{(\ell)}_v = \text{L or S} \end{cases}\] </li> <li><strong>Layer-wise Update</strong>: <ul> <li>A Co-GNN layer involves predicting actions, sampling them, and updating node states.</li> <li>Repeated for L layers to obtain final node representations $\large{h^{(L)}_v}$ .</li> </ul> </li> </ol> <h3 id="environment-network-η-details"><strong>Environment Network η Details</strong></h3> <p>The environment network updates node states using a message-passing scheme based on the selected actions. Let’s consider the standard GCN layer and how it adapts to Co-GNN concepts:</p> <ol> <li><strong>Message Aggregation</strong>: <ul> <li>For each node v , aggregate messages from its neighbors u that are broadcasting or using the standard action. \(m_v^{(\ell)} = \sum_{u \in N_v, a_u^{(\ell)}\ =\ \text{S or B}} h_u^{(\ell)}\)</li> </ul> </li> <li><strong>Node Update</strong>: <ul> <li>The node updates its state based on the aggregated messages and its current state. \(h_v^{(\ell+1)} = \sigma \left( W^{(\ell)}_s h_v^{(\ell)} + W^{(\ell)}_n m_v^{(\ell)} \right)\)</li> </ul> </li> </ol> <h3 id="properties-and-benefits"><strong>Properties and Benefits</strong></h3> <ul> <li><strong>Task-specific</strong>: Nodes learn to focus on relevant neighbors based on the task.</li> <li><strong>Directed</strong>: Edges can become directed, influencing directional information flow.</li> <li><strong>Dynamic and Feature-based</strong>: Adapt to changing graph structures and node features.</li> <li><strong>Asynchronous Updates</strong>: Nodes can be updated independently.</li> <li><strong>Expressive Power</strong>: More expressive than traditional GNNs, capable of handling long-range dependencies and reducing over-squashing and over-smoothing.</li> </ul> <h3 id="example-implementation"><strong>Example Implementation</strong></h3> <p>Consider a GCN (Graph Convolutional Network) adapted with Co-GNN concepts:</p> <ol> <li> <p><strong>GCN Layer (Traditional)</strong>:</p> \[h^{(\ell+1)}_v = \sigma \left( W^{(\ell)}_s h^{(\ell)}_v + W^{(\ell)}_n \sum_{u \in N_v} h^{(\ell)}_u \right)\] </li> <li> <p><strong>Co-GNN Layer</strong>:</p> <ul> <li><strong>Action Network</strong>: Predicts action probabilities for each node.</li> </ul> \[p^{(\ell)}_v = \text{Softmax} \left( W_a h^{(\ell)}_v + b_a \right)\] <ul> <li> <p><strong>Action Sampling</strong>: Gumbel-softmax to select actions. \(a^{(\ell)}_v \sim \text{Gumbel-Softmax}(p^{(\ell)}_v)\)</p> </li> <li> <p><strong>State Update (Environment Network)</strong>:</p> </li> </ul> \[h^{(\ell+1)}_v = \begin{cases} \sigma \left( W^{(\ell)}_s h^{(\ell)}_v \right) &amp; \text{if } a^{(\ell)}_v = \text{I or B} \\ \sigma \left( W^{(\ell)}_s h^{(\ell)}_v + W^{(\ell)}_n \sum_{u \in N_v, a^{(\ell)}_u = \text{S or B}} h^{(\ell)}_u \right) &amp; \text{if } a^{(\ell)}_v = \text{L or S} \end{cases}\] </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Co-GNNs represent a significant advancement in GNN architectures, offering a dynamic and adaptive message-passing framework that improves the handling of complex graph structures and long-range dependencies. The introduction of the Action Network and Environment Network provides a more flexible and task-specific approach to node state updates, leading to superior performance on various graph-related tasks.</p> <p>For further details, refer to the <a href="https://arxiv.org/abs/2310.01267">manuscript</a>.</p> <h1 id="integrating-co-gnn-concepts-into-mace">Integrating Co-GNN Concepts into MACE</h1> <h4 id="1-node-representation"><strong>1. Node Representation</strong></h4> <p>Each node i is represented by:</p> \[\large{\sigma_i^{(t)} = (r_i, z_i, h_i^{(t)})}\] <p>where $r_i \in \mathbb{R}^3$ is the position, $z_i$ is the chemical element, and $h_i^{(t)}$ are the learnable features at layer $t$.</p> <h4 id="2-action-network-π"><strong>2. Action Network (π)</strong></h4> <p>For each atom i at layer t, the Action Network $\pi$ predicts a probability distribution over actions {S, L, B, I}:</p> \[\large{p_i^{(t)} = \pi(\sigma_i^{(t)}, \{\sigma_j^{(t)} | j \in N(i)\})}\] <h4 id="3-action-sampling"><strong>3. Action Sampling</strong></h4> <p>Actions are sampled using the Straight-through Gumbel-softmax estimator:</p> \[\large{a_i^{(t)} \sim \text{Gumbel-Softmax}(p_i^{(t)})}\] <h4 id="4-message-construction"><strong>4. Message Construction</strong></h4> <p>Messages are constructed hierarchically using body order expansion, modified to consider only neighbors that are broadcasting (B) or using the standard action (S):</p> \[\large{m_i^{(t)} = \sum_{j \in N(i), a_j^{(t)} \in \{S, B\}} u_1(\sigma_i^{(t)}, \sigma_j^{(t)}) + \sum_{j_1, j_2 \in N(i), a_{j_1}^{(t)} \in \{S, B\}, a_{j_2}^{(t)} \in \{S, B\}} u_2(\sigma_i^{(t)}, \sigma_{j_1}^{(t)}, \sigma_{j_2}^{(t)}) + \cdots + \sum_{j_1, \ldots, j_\nu \in N(i), a_{j_1}^{(t)} \in \{S, B\}, \ldots, a_{j_\nu}^{(t)} \in \{S, B\}} u_\nu(\sigma_i^{(t)}, \sigma_{j_1}^{(t)}, \ldots, \sigma_{j_\nu}^{(t)})}\] <p>For the two-body interactions:</p> \[\large{A_i^{(t)} = \sum_{j \in N(i), a_j^{(t)} \in \{S, B\}} R_{kl_1l_2l_3}^{(t)}(r_{ij}) Y_{l_1}^{m_1}(\hat{r}_{ij}) W_{kk_2l_2}^{(t)} h_{j,k_2l_2m_2}^{(t)}}\] <p>where R is a learnable radial basis, Y are spherical harmonics, and W are learnable weights.</p> <h4 id="5-higher-order-feature-construction"><strong>5. Higher-order Feature Construction</strong></h4> <p>Higher-order features are constructed using tensor products and symmetrization, modified to consider the actions of neighboring atoms:</p> \[\large{B_{i, \eta \nu k LM}^{(t)} = \sum_{lm} C_{LM \eta \nu, lm} \prod_{\xi=1}^\nu \sum_{k_\xi} w_{kk_\xi l_\xi}^{(t)} A_{i, k_\xi l_\xi m_\xi}^{(t)}}\] <p>where C are generalized Clebsch-Gordan coefficients.</p> <h4 id="6-state-update-environment-network-η"><strong>6. State Update (Environment Network η)</strong></h4> <p>The state update is modified based on the sampled actions:</p> <ul> <li>If $a_i^{(t)} \in {L, S}$:</li> </ul> \[\large{h_i^{(t+1)} = \eta^{(t)}(h_i^{(t)}, \{h_j^{(t)} | j \in N(i), a_j^{(t)} \in \{S, B\}\})}\] <ul> <li>If $a_i^{(t)} \in {I, B}$:</li> </ul> \[\large{h_i^{(t+1)} = \eta^{(t)}(h_i^{(t)}, \{\})}\] <h4 id="7-readout-phase"><strong>7. Readout Phase</strong></h4> <p>In the readout phase, invariant features are mapped to site energies:</p> \[\large{E_i = E_i^{(0)} + E_i^{(1)} + \cdots + E_i^{(T)}}\] <p>where:</p> \[\large{E_i^{(t)} = R_t(h_i^{(t)}) = \sum_{k'} W_{\text{readout}, k'}^{(t)} h_{i, k' 00}^{(t)} \quad \text{for } t &lt; T}\] \[\large{E_i^{(T)} = \text{MLP}_{\text{readout}}^{(t)}(\{h_{i, k 00}^{(t)}\})}\] <h4 id="8-equivariance"><strong>8. Equivariance</strong></h4> <p>The model ensures equivariance under rotation $Q \in O(3)$ :</p> \[\large{h_i^{(t)}(Q \cdot (r_1, \ldots, r_N)) = D(Q) h_i^{(t)}(r_1, \ldots, r_N)}\] <p>where $D(Q)$ is a Wigner D-matrix. For feature $\large{h_{i, k LM}^{(t)}}$ , it transforms as:</p> \[\large{h_{i, k LM}^{(t)}(Q \cdot (r_1, \ldots, r_N)) = \sum_{M'} D_L(Q)_{M'M} h_{i, k LM'}^{(t)}(r_1, \ldots, r_N)}\] <h3 id="conclusion-1">Conclusion</h3> <p>By incorporating the dynamic message-passing strategy of Co-GNNs into the MACE framework, we can enhance its flexibility and adaptability. This involves using an Action Network to determine the message-passing strategy for each atom, modifying the message construction and state update equations accordingly. This integration retains the equivariance properties of MACE while potentially improving its expressiveness and ability to capture complex interactions(?).</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="machine"/><category term="learning"/><category term="potential"/><summary type="html"><![CDATA[Some crucial ideas from Finklestein et al.'s work on Cooperative GNNs]]></summary></entry><entry><title type="html">E(3) - equivariant GNN with Learnable Activation Functions on Edges</title><link href="https://utksi.github.io/blog/2024/gnn_kan/" rel="alternate" type="text/html" title="E(3) - equivariant GNN with Learnable Activation Functions on Edges"/><published>2024-04-21T22:31:00+00:00</published><updated>2024-04-21T22:31:00+00:00</updated><id>https://utksi.github.io/blog/2024/gnn_kan</id><content type="html" xml:base="https://utksi.github.io/blog/2024/gnn_kan/"><![CDATA[<ul> <li>KANs proposed by <a href="https://arxiv.org/abs/2404.19756">Liu et al.</a>.</li> <li>See <a href="https://github.com/GistNoesis/FourierKAN">Fourier-KAN</a> implementation, replaces splines with fourier coefficients.</li> </ul> <h2 id="general-message-passing-neural-network-mpnn">General Message Passing Neural Network (MPNN)</h2> <ol> <li> <p><strong>Input Node and Edge Features</strong>:</p> <ul> <li>Nodes: $\mathbf{x}_i$ (node features)</li> <li>Edges: $\mathbf{e}_{ij}$ (edge features)</li> </ul> </li> <li> <p><strong>Message Passing Layer</strong> (per layer):</p> <p>a. <strong>Edge Feature Transformation</strong>:</p> \[\mathbf{e}'_{ij} = f_e(\mathbf{e}_{ij})\] <p>where $f_e$ is a transformation function applied to edge features.</p> <p>b. <strong>Message Computation</strong>:</p> \[\mathbf{m}_{ij} = f_m(\mathbf{x}_i, \mathbf{x}_j, \mathbf{e}'_{ij})\] <p>where $f_m$ computes messages using node features $\mathbf{x_i} ,\ \mathbf{x_j}$, and transformed edge features $\mathbf{e}’_{ij}$.</p> <p>c. <strong>Message Aggregation</strong>:</p> \[\mathbf{m}_i = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}\] <p>where $\mathcal{N}(i)$ denotes the set of neighbors of node $i$.</p> <p>d. <strong>Node Feature Update</strong>:</p> \[\mathbf{x}'_i = f_n(\mathbf{x}_i, \mathbf{m}_i)\] <p>where $f_n$ updates node features using the aggregated messages $\mathbf{m}_i$.</p> </li> <li> <p><strong>Output Node and Edge Features</strong>:</p> <ul> <li>Nodes: $\mathbf{x}’_i$ (updated node features)</li> <li>Edges: $\mathbf{e}’_{ij}$ (updated edge features)</li> </ul> </li> </ol> <h2 id="e3-equivariant-gnn-with-learnable-activation-functions-on-edges">E3-Equivariant GNN with Learnable Activation Functions on Edges</h2> <ol> <li> <p><strong>Input Node and Edge Features</strong>:</p> <ul> <li>Nodes: $\mathbf{x}_i$ (node features)</li> <li>Edges: $\mathbf{e}_{ij}$ (edge features)</li> </ul> </li> <li> <p><strong>Learnable Edge Feature Transformation</strong>:</p> <ul> <li> <p><strong>Fourier-based Edge Transformation</strong>:</p> \[\mathbf{e}'_{ij} = \text{FourierTransform}(\mathbf{e}_{ij})\] <p>where the Fourier transformation is applied to edge features. Specifically, the transformation is defined as:</p> \[\mathbf{e}'_{ij} = \sum_{k=1}^{K} a_{ij,k} \cos(k \mathbf{e}_{ij}) + b_{ij,k} \sin(k \mathbf{e}_{ij})\] <p>Here, $a_{ij,k}$ and $b_{ij,k}$ are learnable parameters, and $K$ is the number of Fourier terms.</p> </li> </ul> </li> <li> <p><strong>Message Passing and Aggregation</strong>:</p> <p>a. <strong>Message Computation</strong>:</p> \[\mathbf{m}_{ij} = \mathbf{e}'_{ij} \odot \mathbf{x}_j\] <p>where $\odot$ denotes element-wise multiplication, combining the transformed edge features $\mathbf{e}’_{ij}$ with the neighboring node features $\mathbf{x}_j$.</p> <p>b. <strong>Message Aggregation</strong>:</p> \[\mathbf{m}_i = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}\] <p>c. <strong>Simple Node Feature Transformation</strong>:</p> \[\mathbf{x}'_i = \mathbf{W} (\mathbf{x}_i + \mathbf{m}_i) + \mathbf{b}\] <p>where $\mathbf{W}$ is a learnable weight matrix and $\mathbf{b}$ is a bias vector.</p> </li> <li> <p><strong>Output Node and Edge Features</strong>:</p> <ul> <li>Nodes: $\mathbf{x}’_i$ (updated node features)</li> <li>Edges: $\mathbf{e}’_{ij}$ (updated edge features)</li> </ul> </li> </ol> <h2 id="full-implementation">Full Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch_scatter</span> <span class="kn">import</span> <span class="n">scatter_add</span>
<span class="kn">from</span> <span class="n">torch_geometric.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">QM9</span>
<span class="kn">from</span> <span class="n">torch_geometric.transforms</span> <span class="kn">import</span> <span class="n">Distance</span>
<span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">MessagePassing</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="n">e3nn</span> <span class="kn">import</span> <span class="n">o3</span>
<span class="kn">from</span> <span class="n">e3nn.nn</span> <span class="kn">import</span> <span class="n">Gate</span><span class="p">,</span> <span class="n">FullyConnectedNet</span>

  
<span class="k">class</span> <span class="nc">LearnableActivationEdge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

Class to define learnable activation functions on edges using Fourier series.

Inspired by Kolmogorov-Arnold Networks (KANs) to capture complex, non-linear transformations on edge features.

</span><span class="sh">"""</span>

<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputdim</span><span class="p">,</span> <span class="n">outdim</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">,</span> <span class="n">addbias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

Initialize the LearnableActivationEdge module.

Args:

inputdim (int): Dimension of input edge features.

outdim (int): Dimension of output edge features.

num_terms (int): Number of Fourier terms.

addbias (bool): Whether to add a bias term. Default is True.

</span><span class="sh">"""</span>

<span class="nf">super</span><span class="p">(</span><span class="n">LearnableActivationEdge</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

<span class="n">self</span><span class="p">.</span><span class="n">num_terms</span> <span class="o">=</span> <span class="n">num_terms</span>

<span class="n">self</span><span class="p">.</span><span class="n">addbias</span> <span class="o">=</span> <span class="n">addbias</span>

<span class="n">self</span><span class="p">.</span><span class="n">inputdim</span> <span class="o">=</span> <span class="n">inputdim</span>

<span class="n">self</span><span class="p">.</span><span class="n">outdim</span> <span class="o">=</span> <span class="n">outdim</span>

  

<span class="c1"># Initialize learnable Fourier coefficients
</span>
<span class="n">self</span><span class="p">.</span><span class="n">fouriercoeffs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">outdim</span><span class="p">,</span> <span class="n">inputdim</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">)</span> <span class="o">/</span>

<span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">inputdim</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">num_terms</span><span class="p">))))</span>

<span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">addbias</span><span class="p">:</span>

<span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">outdim</span><span class="p">))</span>

  

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

Forward pass to apply learnable activation functions on edge attributes.

Args:

edge_attr (Tensor): Edge attributes of shape (..., inputdim).

Returns:

Tensor: Transformed edge attributes of shape (..., outdim).

</span><span class="sh">"""</span>

<span class="c1"># Reshape edge attributes for Fourier transformation
</span>
<span class="n">xshp</span> <span class="o">=</span> <span class="n">edge_attr</span><span class="p">.</span><span class="n">shape</span>

<span class="n">outshape</span> <span class="o">=</span> <span class="n">xshp</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">outdim</span><span class="p">,)</span>

<span class="n">edge_attr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">inputdim</span><span class="p">))</span>

  

<span class="c1"># Generate Fourier terms
</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_terms</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">edge_attr</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_terms</span><span class="p">))</span>

<span class="n">xrshp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="n">edge_attr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

  

<span class="c1"># Compute cosine and sine components
</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">xrshp</span><span class="p">)</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">xrshp</span><span class="p">)</span>

  

<span class="c1"># Apply learnable Fourier coefficients
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">fouriercoeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="n">y</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">fouriercoeffs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

  

<span class="c1"># Add bias if applicable
</span>
<span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">addbias</span><span class="p">:</span>

<span class="n">y</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>

  

<span class="c1"># Reshape to original edge attribute shape
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">outshape</span><span class="p">)</span>

<span class="k">return</span> <span class="n">y</span>

  

<span class="k">class</span> <span class="nc">E3EquivariantGNN</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

E(3)-Equivariant Graph Neural Network (GNN) that focuses on learnable activation functions on edges.

</span><span class="sh">"""</span>

<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

Initialize the E3EquivariantGNN module.

Args:

in_features (int): Dimension of input node features.

out_features (int): Dimension of output node features.

hidden_dim (int): Dimension of hidden layers.

num_layers (int): Number of layers in the network.

num_terms (int): Number of Fourier terms for learnable activation functions.

</span><span class="sh">"""</span>

<span class="nf">super</span><span class="p">(</span><span class="n">E3EquivariantGNN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

<span class="c1"># Define the input and output irreps (representations)
</span>
<span class="n">self</span><span class="p">.</span><span class="n">input_irrep</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="n">Irreps</span><span class="p">.</span><span class="nf">spherical_harmonics</span><span class="p">(</span><span class="n">lmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Example irreps, adjust as needed
</span>
<span class="n">self</span><span class="p">.</span><span class="n">output_irrep</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="nc">Irreps</span><span class="p">([(</span><span class="n">out_features</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))])</span> <span class="c1"># Scalar output
</span>
<span class="c1"># Define the hidden irreps
</span>
<span class="n">hidden_irreps</span> <span class="o">=</span> <span class="p">[</span><span class="n">o3</span><span class="p">.</span><span class="n">Irreps</span><span class="p">.</span><span class="nf">spherical_harmonics</span><span class="p">(</span><span class="n">lmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="c1"># Adjust as needed
</span>
<span class="c1"># Create the equivariant layers and learnable activation functions on edges
</span>
<span class="n">self</span><span class="p">.</span><span class="n">fourier_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>

<span class="nc">LearnableActivationEdge</span><span class="p">(</span><span class="n">in_features</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>

<span class="p">])</span>

<span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>

<span class="nc">Gate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_irrep</span><span class="p">,</span> <span class="n">hidden_irreps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">num_terms</span><span class="p">),</span>

<span class="o">*</span><span class="p">[</span><span class="nc">Gate</span><span class="p">(</span><span class="n">hidden_irreps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden_irreps</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">num_terms</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span>

<span class="nc">Gate</span><span class="p">(</span><span class="n">hidden_irreps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">output_irrep</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">num_terms</span><span class="p">)</span>

<span class="p">])</span>

<span class="c1"># Output layer
</span>
<span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

  

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

Forward pass to propagate node features through the GNN.

Args:

x (Tensor): Node features of shape (num_nodes, in_features).

edge_index (Tensor): Edge indices of shape (2, num_edges).

edge_attr (Tensor): Edge attributes of shape (num_edges, edge_dim).

Returns:

Tensor: Output node features of shape (num_nodes, out_features).

</span><span class="sh">"""</span>

<span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">edge_index</span>

  

<span class="c1"># Apply Fourier-based message passing and equivariant transformations
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>

<span class="c1"># Transform edge features with Fourier series
</span>
<span class="n">fourier_messages</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">fourier_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">edge_attr</span><span class="p">)</span>

<span class="c1"># Apply equivariant transformations to node features
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">fourier_messages</span><span class="p">)</span>

<span class="c1"># Compute messages
</span>
<span class="n">m_ij</span> <span class="o">=</span> <span class="n">fourier_messages</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>

<span class="c1"># Aggregate messages
</span>
<span class="n">m_i</span> <span class="o">=</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">m_ij</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim_size</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># Update node features
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">m_i</span>

  

<span class="c1"># Apply the final linear layer
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">return</span> <span class="n">x</span>

<span class="c1"># Load and prepare the QM9 dataset
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nc">QM9</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/QM9</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="nc">Distance</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  

<span class="c1"># Split dataset into training, validation, and test sets
</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:</span><span class="mi">110000</span><span class="p">]</span>

<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">110000</span><span class="p">:</span><span class="mi">120000</span><span class="p">]</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">120000</span><span class="p">:]</span>

  

<span class="c1"># Data loaders for training, validation, and test sets
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">val_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  

<span class="c1"># Define the loss function and optimizer
</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">E3EquivariantGNN</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_terms</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

  

<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>

<span class="sh">"""</span><span class="s">

Perform a single training step.

Args:

model (nn.Module): The neural network model.

optimizer (Optimizer): The optimizer.

criterion (Loss): The loss function.

data (Data): The input data batch.

Returns:

float: The loss value.

</span><span class="sh">"""</span>

<span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

<span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_attr</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>

<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

  

<span class="c1"># Training loop
</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>

<span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>

<span class="n">train_loss</span> <span class="o">+=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

<span class="n">train_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

  

<span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>

<span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_attr</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>

<span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

<span class="n">val_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span>

  

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">, Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Val Loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h2 id="detailed-explanation-of-mathematical-formulations">Detailed Explanation of Mathematical Formulations</h2> <h3 id="learnable-edge-feature-transformation">Learnable Edge Feature Transformation</h3> <p>For each edge $(i, j)$ with feature $\mathbf{e}_{ij}$:</p> \[\mathbf{e}'_{ij} = \sum_{k=1}^{K} a_{ij,k} \cos(k \mathbf{e}_{ij}) + b_{ij,k} \sin(k \mathbf{e}_{ij})\] <p>where $a_{ij,k}$ and $b_{ij,k}$ are learnable parameters, and $K$ is the number of terms.</p> <h3 id="message-computation">Message Computation</h3> <p>For each edge $(i, j)$:</p> \[\mathbf{m}_{ij} = \mathbf{e}'_{ij} \odot \mathbf{x}_j\] <p>where $\odot$ denotes element-wise multiplication.</p> <h3 id="message-aggregation">Message Aggregation</h3> <p>For each node $i$:</p> \[\mathbf{m}_i = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}\] <p>where $\mathcal{N}(i)$ denotes the set of neighbors of node $i$.</p> <h3 id="node-feature-update">Node Feature Update</h3> <p>For each node $i$:</p> \[\mathbf{x}'_i = \mathbf{W} (\mathbf{x}_i + \mathbf{m}_i) + \mathbf{b}\] <p>where $\mathbf{W}$ is a learnable weight matrix and $\mathbf{b}$ is a bias vector.</p> <h2 id="summary">Summary</h2> <p>This implementation combines the learnable activation functions on edges with E(3) equivariant transformations on node features. The detailed mathematical formulations provided in the comments explain each step of the process, making it suitable for a physicist audience familiar with these concepts.</p> <p>#Idea #TODO: KANs for learnable edge activations in MACE - to have it as an option. Train on the same set.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[Some ideas about KAN based GNNs beyond just stacking layers]]></summary></entry><entry><title type="html">MACE (Message Passing ACE)</title><link href="https://utksi.github.io/blog/2023/mace/" rel="alternate" type="text/html" title="MACE (Message Passing ACE)"/><published>2023-08-01T22:36:00+00:00</published><updated>2023-08-01T22:36:00+00:00</updated><id>https://utksi.github.io/blog/2023/mace</id><content type="html" xml:base="https://utksi.github.io/blog/2023/mace/"><![CDATA[<h3 id="introduction"><strong>Introduction</strong></h3> <p>MACE (Message Passing Atomic Cluster Expansion) is an equivariant message passing neural network that uses higher-order messages to enhance the accuracy and efficiency of force fields in computational chemistry.</p> <h3 id="node-representation"><strong>Node Representation</strong></h3> <p>Each node $\large{i}$ is represented by:</p> \[\large{\sigma_i^{(t)} = (r_i, z_i, h_i^{(t)})}\] <p>where $r_i \in \mathbb{R}^3$ is the position, $\large{z_i}$ is the chemical element, and $\large{h_i^{(t)}}$ are the learnable features at layer $\large{t}$.</p> <h3 id="message-construction"><strong>Message Construction</strong></h3> <p>Messages are constructed hierarchically using a body order expansion:</p> \[m_i^{(t)} = \sum_j u_1(\sigma_i^{(t)}, \sigma_j^{(t)}) + \sum_{j_1, j_2} u_2(\sigma_i^{(t)}, \sigma_{j_1}^{(t)}, \sigma_{j_2}^{(t)}) + \cdots + \sum_{j_1, \ldots, j_\nu} u_\nu(\sigma_i^{(t)}, \sigma_{j_1}^{(t)}, \ldots, \sigma_{j_\nu}^{(t)})\] <h3 id="two-body-message-construction"><strong>Two-body Message Construction</strong></h3> <p>For two-body interactions, the message $m_i^{(t)}$ is:</p> \[A_i^{(t)} = \sum_{j \in N(i)} R_{kl_1l_2l_3}^{(t)}(r_{ij}) Y_{l_1}^{m_1}(\hat{r}_{ij}) W_{kk_2l_2}^{(t)} h_{j,k_2l_2m_2}^{(t)}\] <p>where $\large{R}$ is a learnable radial basis, $\large{Y}$ are spherical harmonics, and $\large{W}$ are learnable weights. $\large{C}$ are Clebsch-Gordan coefficients ensuring equivariance.</p> <h3 id="higher-order-feature-construction"><strong>Higher-order Feature Construction</strong></h3> <p>Higher-order features are constructed using tensor products and symmetrization:</p> \[\large{B_{i, \eta \nu k LM}^{(t)} = \sum_{lm} C_{LM \eta \nu, lm} \prod_{\xi=1}^\nu \sum_{k_\xi} w_{kk_\xi l_\xi}^{(t)} A_{i, k_\xi l_\xi m_\xi}^{(t)}}\] <p>where $\large{C}$ are generalized Clebsch-Gordan coefficients.</p> <h3 id="message-passing"><strong>Message Passing</strong></h3> <p>The message passing updates the node features by aggregating messages:</p> \[\large{h_i^{(t+1)} = U_{kL}^{(t)}(\sigma_i^{(t)}, m_i^{(t)}) = \sum_{k'} W_{kL, k'}^{(t)} m_{i, k' LM} + \sum_{k'} W_{z_i kL, k'}^{(t)} h_{i, k' LM}^{(t)}}\] <h3 id="readout-phase"><strong>Readout Phase</strong></h3> <p>In the readout phase, invariant features are mapped to site energies:</p> \[\large{E_i = E_i^{(0)} + E_i^{(1)} + \cdots + E_i^{(T)}}\] <p>where:</p> \[\large{E_i^{(t)} = R_t(h_i^{(t)}) = \sum_{k'} W_{\text{readout}, k'}^{(t)} h_{i, k' 00}^{(t)} \quad \text{for } t &lt; T}\] \[\large{E_i^{(T)} = \text{MLP}_{\text{readout}}^{(t)}(\{h_{i, k 00}^{(t)}\})}\] <h3 id="equivariance"><strong>Equivariance</strong></h3> <p>The model ensures equivariance under rotation $\large{Q \in O(3)}$ :</p> \[\large{h_i^{(t)}(Q \cdot (r_1, \ldots, r_N)) = D(Q) h_i^{(t)}(r_1, \ldots, r_N)}\] <p>where $\large{D(Q)}$ is a Wigner D-matrix. For feature $\large{h_{i, k LM}^{(t)}}$, it transforms as:</p> \[\large{h_{i, k LM}^{(t)}(Q \cdot (r_1, \ldots, r_N)) = \sum_{M'} D_L(Q)_{M'M} h_{i, k LM'}^{(t)}(r_1, \ldots, r_N)}\] <h2 id="properties-and-computational-efficiency">Properties and Computational Efficiency</h2> <ol> <li><strong>Body Order Expansion</strong>: <ul> <li>MACE constructs messages using higher body order expansions, enabling rich representations of atomic environments.</li> </ul> </li> <li><strong>Computational Efficiency</strong>: <ul> <li>The use of higher-order messages reduces the required number of message-passing layers to two, enhancing computational efficiency and scalability.</li> </ul> </li> <li><strong>Receptive Field</strong>: <ul> <li>MACE maintains a small receptive field by decoupling correlation order increase from the number of message-passing iterations, facilitating parallelization.</li> </ul> </li> <li><strong>State-of-the-Art Performance</strong>: <ul> <li>MACE achieves state-of-the-art accuracy on benchmark tasks (rMD17, 3BPA, AcAc), demonstrating its effectiveness in modeling complex atomic interactions.</li> </ul> </li> </ol> <p>For further details, refer to the <a href="https://arxiv.org/abs/2206.07697">Batatia et al.</a>.</p> <h2 id="necessary-math-to-know">Necessary math to know:</h2> <h3 id="1-spherical-harmonics">1. <strong>Spherical Harmonics</strong></h3> <p><strong>Concept:</strong></p> <ul> <li>Spherical harmonics $Y^L_M$ are functions defined on the surface of a sphere. They are used in many areas of physics, including quantum mechanics and electrodynamics, to describe the angular part of a system.</li> </ul> <p><strong>Role in MACE:</strong></p> <ul> <li>Spherical harmonics are used to decompose the angular dependency of the atomic environment. This helps in capturing the rotational properties of the features in a systematic way.</li> </ul> <p><strong>Mathematically:</strong></p> <ul> <li>The spherical harmonics $Y^L_M(\theta, \phi)$ are given by:</li> </ul> \[Y^L_M(\theta, \phi) = \sqrt{\frac{(2L+1)}{4\pi} \frac{(L-M)!}{(L+M)!}} P^M_L(\cos \theta) e^{iM\phi}\] <p>where $P^M_L$ are the associated Legendre polynomials.</p> <h3 id="2-clebsch-gordan-coefficients">2. <strong>Clebsch-Gordan Coefficients</strong></h3> <p><strong>Concept:</strong></p> <ul> <li>Clebsch-Gordan coefficients are used in quantum mechanics to combine angular momenta. They arise in the coupling of two angular momentum states to form a new angular momentum state.</li> </ul> <p><strong>Role in MACE:</strong></p> <ul> <li>In MACE, Clebsch-Gordan coefficients are used to combine features from different atoms while maintaining rotational invariance. They ensure that the resulting features transform correctly under rotations, preserving the physical symmetry of the system.</li> </ul> <p><strong>Mathematically:</strong></p> <ul> <li>When combining two angular momentum states $\vert l_1, m_1\rangle$ and $\vert l_2, m_2\rangle$, the resulting state $\vert L, M\rangle$ is given by:</li> </ul> \[|L, M\rangle = \sum_{m_1, m_2} C_{L, M}^{l_1, m_1; l_2, m_2} |l_1, m_1\rangle |l_2, m_2\rangle\] <p>where $C_{L, M}^{l_1, m_1; l_2, m_2}$ are the Clebsch-Gordan coefficients.</p> <h3 id="3-o3-rotations">3. <strong>$O(3)$ Rotations</strong></h3> <p><strong>Concept:</strong></p> <ul> <li>The group $O(3)$ consists of all rotations and reflections in three-dimensional space. It represents the symmetries of a 3D system, including operations that preserve the distance between points.</li> </ul> <p><strong>Role in MACE:</strong></p> <ul> <li>Ensuring that the neural network respects $O(3)$ symmetry is crucial for modeling physical systems accurately. MACE achieves this by using operations that are invariant or equivariant under these rotations and reflections.</li> </ul> <p><strong>Mathematically:</strong></p> <ul> <li>A rotation in $O(3)$ can be represented by a 3x3 orthogonal matrix $Q$ such that:</li> </ul> \[Q^T Q = I \quad \text{and} \quad \det(Q) = \pm 1\] <p>where $I$ is the identity matrix.</p> <h3 id="4-wigner-d-matrix">4. <strong>Wigner D-matrix</strong></h3> <p><strong>Concept:</strong></p> <ul> <li>The Wigner D-matrix $D^L(Q)$ represents the action of a rotation $Q$ on spherical harmonics. It provides a way to transform the components of a tensor under rotation.</li> </ul> <p><strong>Role in MACE:</strong></p> <ul> <li>Wigner D-matrices are used to ensure that the feature vectors in the neural network transform correctly under rotations. This is essential for maintaining the rotational equivariance of the model.</li> </ul> <p><strong>Mathematically:</strong></p> <ul> <li>For a rotation $Q \in O(3)$ and a spherical harmonic of degree $L$, the Wigner D-matrix $D^L(Q)$ is a $(2L+1) \times (2L+1)$ matrix. If $Y^L_M$ is a spherical harmonic, then under rotation $Q$, it transforms as:</li> </ul> \[Y^L_M(Q \cdot \mathbf{r}) = \sum_{M'=-L}^{L} D^L_{M'M}(Q) Y^L_{M'}(\mathbf{r})\]]]></content><author><name></name></author><category term="sample-posts"/><category term="machine"/><category term="learning"/><category term="potential"/><summary type="html"><![CDATA[A summary of Message Passing Atomic Cluster Expansion Graph Neural Networks]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://utksi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://utksi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://utksi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>