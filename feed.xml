<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://utksi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://utksi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-15T18:47:02+00:00</updated><id>https://utksi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">[Stuff I read] Dark states of electrons in a quantum system with two pairs of sublattices</title><link href="https://utksi.github.io/blog/2024/dark_states/" rel="alternate" type="text/html" title="[Stuff I read] Dark states of electrons in a quantum system with two pairs of sublattices"/><published>2024-09-13T12:59:44+00:00</published><updated>2024-09-13T12:59:44+00:00</updated><id>https://utksi.github.io/blog/2024/dark_states</id><content type="html" xml:base="https://utksi.github.io/blog/2024/dark_states/"><![CDATA[<ul> <li>A slightly different way of looking at dark bands in the Brillouin Zone (BZ). See <a href="https://doi.org/10.1038/s41567-024-02586-x">Chung et al.</a>.</li> </ul> <p>This review discusses the discovery of <strong>condensed-matter dark states</strong> in the material <strong>Palladium Diselenide (PdSe₂)</strong>, where entire bands of quantum states in the Brillouin zone remain undetectable via angle-resolved photoemission spectroscopy (ARPES). The dark states arise due to interference effects between sublattices, a novel feature in quantum materials.</p> <hr/> <h2 id="1-introduction-to-dark-states">1. Introduction to Dark States</h2> <p>In quantum mechanics, a <strong>dark state</strong> refers to a state that cannot absorb or emit photons and is thus undetectable through typical spectroscopic methods. These states are typically well-known in atomic and molecular systems where they arise due to <strong>quantum interference</strong> or <strong>conservation of angular momentum</strong>.</p> <p>In the condensed-matter context, dark states have been less explored, especially when caused by interference between <strong>sublattices</strong>. This paper expands the concept to solid-state systems, where dark states emerge due to destructive interference within the crystal’s sublattices. These states remain hidden from ARPES measurements because their transition matrix elements vanish.</p> <h3 id="key-definitions">Key Definitions:</h3> <ul> <li><strong>Dark State</strong>: A quantum state that does not interact with light and is therefore undetectable by traditional spectroscopic methods.</li> <li><strong>Quantum Interference</strong>: The phenomenon where the probability amplitudes of quantum states add or cancel out, affecting the visibility of quantum transitions.</li> </ul> <hr/> <h2 id="2-crystal-structure-and-sublattice-symmetry">2. Crystal Structure and Sublattice Symmetry</h2> <p>PdSe₂ is chosen for its crystal structure, which consists of two pairs of <strong>sublattices</strong> labeled ( A ), ( B ), ( C ), and ( D ). These sublattices are related by <strong>glide-mirror symmetries</strong>, which leads to specific <strong>quantum phases</strong> that control the interference patterns of electronic wavefunctions in the Brillouin zone.</p> <p>Mathematically, the electronic structure of PdSe₂ is described using the <strong>tight-binding Hamiltonian</strong> model. The dominant states arise from <strong>Pd 4d orbitals</strong>, and the relative phases ( \varphi<em>{AB}, \varphi</em>{AC}, \varphi_{AD} ) between sublattices dictate whether the interference is constructive or destructive:</p> <p>[ H<em>{PdSe_2} = \begin{pmatrix} f</em>{AA} &amp; f<em>{AB} &amp; f</em>{AC} &amp; f<em>{AD} <br/> f</em>{AB} &amp; f<em>{AA} &amp; f</em>{AC} &amp; f<em>{AD} <br/> f</em>{AC} &amp; f<em>{AD} &amp; f</em>{AA} &amp; f<em>{AB} <br/> f</em>{AD} &amp; f<em>{AC} &amp; f</em>{AB} &amp; f_{AA} \end{pmatrix} ]</p> <p>The key discovery is that PdSe₂ has a unique sublattice arrangement where <strong>multiple glide-mirror symmetries</strong> connect these sublattices, resulting in <strong>double destructive interference</strong> under certain conditions, leading to the appearance of <strong>dark states</strong>.</p> <h3 id="key-definitions-1">Key Definitions:</h3> <ul> <li><strong>Sublattice</strong>: A subset of atoms within a crystal lattice that repeats in a regular pattern.</li> <li><strong>Glide-Mirror Symmetry</strong>: A symmetry operation combining a reflection with a translation.</li> <li><strong>Tight-Binding Hamiltonian</strong>: A mathematical model used to describe the movement of electrons in a material by considering the hopping between atoms.</li> </ul> <hr/> <h2 id="3-arpes-light-polarization-and-dark-states">3. ARPES, Light Polarization, and Dark States</h2> <p>The experimental technique used in the paper, <strong>ARPES</strong>, allows researchers to probe the electronic band structure of a material. However, in PdSe₂, an entire band of electronic states in the Brillouin zone is <strong>invisible</strong> regardless of the photon energy or light polarization used. This is a clear indicator of <strong>dark states</strong> resulting from <strong>sublattice interference</strong>.</p> <p>The transition probability in ARPES is governed by <strong>Fermi’s Golden Rule</strong>:</p> <p>[ M_k = \int \psi_f^* (\mathbf{A} \cdot \mathbf{p}) \psi_i \, dV ]</p> <p>Where:</p> <ul> <li>( \mathbf{A} ) is the electromagnetic vector potential.</li> <li>( \mathbf{p} ) is the momentum operator.</li> <li>( \psi_i ) and ( \psi_f ) are the initial and final electronic states.</li> </ul> <p>The critical point in PdSe₂ is that the <strong>interference between sublattices</strong> can lead to <strong>destructive interference</strong> when certain relative quantum phases ( \varphi<em>{AB}, \varphi</em>{AC}, \varphi_{AD} ) cancel out the matrix elements ( M_k ). This makes some states completely <strong>undetectable by ARPES</strong>.</p> <hr/> <h2 id="4-phase-polarization-and-brillouin-zone-quantum-states">4. Phase Polarization and Brillouin Zone Quantum States</h2> <p>A major finding in this paper is the identification of <strong>phase polarization</strong> in the Brillouin zone of PdSe₂. The electronic wavefunctions in PdSe₂ are fully polarized to one of four possible states: ( 000, 0\pi\pi, \pi0\pi, \pi\pi0 ), depending on the relative quantum phases ( \varphi<em>{AB}, \varphi</em>{AC}, \varphi_{AD} ).</p> <ul> <li>The <strong>000 state</strong> (blue pseudospin) is <strong>visible</strong> in ARPES under <strong>p-polarized light</strong>, because constructive interference ensures a non-zero matrix element.</li> <li>The other states, ( 0\pi\pi, \pi0\pi, \pi\pi0 ) (red, yellow, and green pseudospins), are <strong>dark states</strong> because two of the three quantum phases are ( \pi ), leading to <strong>double destructive interference</strong>. These states are completely undetectable by ARPES under <strong>any light polarization</strong>.</li> </ul> <hr/>]]></content><author><name></name></author><category term="journal-club"/><category term="physics"/><category term="spectroscopy"/><category term="materials-science"/><summary type="html"><![CDATA[A high-level summary of main ideas from Chung et al.]]></summary></entry><entry><title type="html">[Stuff I read] Phonon induced renormalization of exchange interactions in two-dimensional magnets</title><link href="https://utksi.github.io/blog/2024/renorm/" rel="alternate" type="text/html" title="[Stuff I read] Phonon induced renormalization of exchange interactions in two-dimensional magnets"/><published>2024-06-13T15:59:44+00:00</published><updated>2024-06-13T15:59:44+00:00</updated><id>https://utksi.github.io/blog/2024/renorm</id><content type="html" xml:base="https://utksi.github.io/blog/2024/renorm/"><![CDATA[<ul> <li>Via <a href="https://arxiv.org/abs/2406.05229">Badrtdinov, Katsnelson &amp; Rudenko</a></li> <li>I find this very interesting. There have been recent (relatively) works on introducing spin-lattice interaction in practical implementation(s) of ASD solvers [See the work from Anders Bergman and Anna Delin (2022-2023)], but renormalization is not taken care of in those things explicitly. To get a measure of how much the exchange interactions are affected as a function of temperature independent of explicit electron-phonon interactions, this could be interesting.</li> </ul> <h2 id="first-a-one-line-or-two-d-introduction-to-key-concepts">First, a one line (or two :D) introduction to key concepts</h2> <p><strong>Magnetism in 2D Materials</strong>:</p> <ul> <li><strong>2D Magnets</strong> are materials with magnetic properties confined to two dimensions, influenced significantly by quantum effects. They are promising for applications in spintronics, where electronic spins are used to store, process, and transfer information.</li> </ul> <p><strong>Heisenberg Model</strong>:</p> <ul> <li>Describes magnetic interactions through pairwise exchange interactions.</li> <li>The Hamiltonian: \(H*0 = \sum*{i &gt; j} J*{ij} \mathbf{S}\_i \cdot \mathbf{S}\_j\) where \(J*{ij}\) is the exchange interaction between spins \(\mathbf{S}\_i\) and \(\mathbf{S}\_j\).</li> </ul> <p><strong>Electron-Phonon Coupling</strong>:</p> <ul> <li>Refers to interactions between electrons and lattice vibrations (phonons).</li> <li>These interactions affect various electronic properties, including magnetic exchange interactions.</li> </ul> <p><strong>Green’s Functions and Self-Energy</strong>:</p> <ul> <li><strong>Green’s Functions</strong> \(G^{\sigma}\_{ij}(i\omega_n)\): Describe electron propagation with spin \(\sigma\).</li> <li><strong>Self-Energy</strong> \(\Sigma^{\sigma}\_k(i\omega_n)\): Represents interaction effects on electrons due to phonons and other electrons.</li> </ul> <h2 id="1-theory-and-model">1. Theory and Model</h2> <p>The paper extends the Heisenberg model to include electron-phonon interactions, recalculating the exchange interaction \(J\_{ij}\) using the magnetic force theorem:</p> \[J_{ij} = 2 \text{Tr}_{\omega L} \left[ \Delta_i G^{\uparrow}_{ij}(i\omega_n) \Delta_j G^{\downarrow}_{ji}(i\omega_n) \right] S^{-2}\] <p>where:</p> <ul> <li>\(\Delta_i\) is the exchange splitting at lattice site \(i\).</li> <li>\(G^{\sigma}\_{ij}(i\omega_n)\) is the spin-polarized electron propagator.</li> <li>\(\text{Tr}\_{\omega L}\) denotes the trace over Matsubara frequencies \(i\omega_n\) and orbital indices \(L\).</li> </ul> <p>Incorporating electron-phonon interactions, the Green’s function is renormalized using the Dyson equation:</p> \[G^{-1}_k(i\omega_n) \rightarrow \tilde{G}^{-1}_k(i\omega_n) = G^{-1}_k(i\omega_n) - \Sigma_k(i\omega_n)\] <p>This leads to a renormalized exchange splitting:</p> \[\Delta \rightarrow \tilde{\Delta}_k(i\omega_n) = \Delta + \Sigma^{\uparrow}_k(i\omega_n) - \Sigma^{\downarrow}_k(i\omega_n)\] <p>where the self-energy \(\Sigma^{\sigma}\_k(i\omega_n)\) is given by:</p> \[\Sigma^{\sigma}_k(i\omega_n) = -T \sum_{k' \nu m} G^{\sigma}_{k'}(i\omega_n - i\omega_m) |g^{\nu \sigma}_{kk'}|^2 D_{k-k'}(i\omega_n - i\omega_m)\] <p>Here, \(g^{\nu \sigma}\_{kk'}\) is the electron-phonon coupling vertex, and \(D_q(i\omega_n)\) is the phonon propagator.</p> <p><strong>Context</strong>: This theoretical framework allows the authors to predict how the electron-phonon interactions influence the magnetic properties of 2D materials by renormalizing the exchange interactions between spins.</p> <h2 id="2-square-lattice-model">2. Square Lattice Model</h2> <p>To illustrate the effect, the authors use a square lattice model at half-filling with the Hamiltonian:</p> \[H = t \sum_{\langle ij \rangle \sigma} c^{\dagger}_{i\sigma} c_{j\sigma} + \frac{\Delta}{2} \sum_i (n^{\uparrow}_i - n^{\downarrow}_i) + \sum_q \omega_q b^{\dagger}_q b_q + \sum_{q, \langle ij \rangle \sigma} g_q (b^{\dagger}_q + b_{-q}) c^{\dagger}_{i\sigma} c_{j\sigma}\] <p>where:</p> <ul> <li>\(t\) is the nearest-neighbor hopping.</li> <li>\(\Delta\) is the on-site exchange splitting.</li> <li>\(\omega_q\) is the phonon frequency.</li> <li>\(g_q\) is the electron-phonon coupling constant.</li> </ul> <p>The self-energy in the high-temperature limit simplifies to:</p> \[\Sigma^{\sigma}_k(\omega, T) = 2\lambda \frac{k_BT}{N^{\sigma}_F} \sum_q G^{\sigma}_{k+q}(\omega)\] <p>where \(\lambda\) is the dimensionless electron-phonon coupling constant.</p> <p><strong>Context</strong>: The square lattice model serves as a simple yet effective system to understand the temperature dependence of exchange interactions due to electron-phonon coupling.</p> <h2 id="3-renormalization-of-exchange-interactions">3. Renormalization of Exchange Interactions</h2> <p>The main result shows that the exchange interaction is renormalized linearly with temperature due to electron-phonon coupling:</p> \[J(T) = J(0) - c\lambda T\] <p>where \(c\) is a renormalization constant.</p> <p><strong>Derivation</strong>:</p> <ul> <li>The linear temperature dependence arises from the self-energy correction, which modifies the exchange interaction strength \(J\_{ij}\).</li> <li>The renormalization constant \(c\) is determined by the specific electronic structure of the material and the strength of the electron-phonon coupling.</li> </ul> <h2 id="4-application-to-mathrmfe_3gete_2">4. Application to \(\mathrm{Fe_3GeTe_2}\)</h2> <p>For the metallic 2D ferromagnet \(\mathrm{Fe_3GeTe_2}\), the authors use first-principles calculations to determine the electronic and phononic structures. The temperature dependence of the exchange interactions is calculated, showing a reduction of the Curie temperature by about 10% due to electron-phonon interactions.</p> <p><strong>First-Principles Calculation</strong>:</p> <ul> <li><strong>Density Functional Theory (DFT)</strong> is employed to calculate the electronic structure.</li> <li><strong>Density Functional Perturbation Theory (DFPT)</strong> is used for phonon calculations.</li> <li>The electronic structure in the vicinity of the Fermi level is parameterized using maximally localized Wannier functions.</li> </ul> <p><strong>Context</strong>: Wannierization is an invaluable tool for simply writing the hamiltonian in a local basis in the vicinity of the Fermi level, such a model is easier to solve exactly at lower (read: more rigorous) levels of theory as well, which is essential for evaluating the temperature-dependent exchange interactions.</p> <h2 id="5-spin-wave-renormalization">5. Spin-Wave Renormalization</h2> <p><strong>Spin-Wave Theory</strong>:</p> <ul> <li>Spin waves, or magnons, are collective excitations in a magnetically ordered system.</li> <li>The stability of magnetic order is influenced by spin-wave spectra, which can be calculated by diagonalizing the spin-wave Hamiltonian.</li> </ul> <p>The Heisenberg model with single-ion anisotropy (SIA) is used to describe the spin waves:</p> \[H = H_0 + A \sum_i (S^z_i)^2\] <p>where \(A\) is the anisotropy parameter.</p> <p><strong>Magnon Eigenvectors and Spectra</strong>:</p> <ul> <li>Magnon frequencies \(\Omega\_{q \nu}\) are obtained by diagonalizing the spin-wave Hamiltonian:</li> </ul> \[H^{\text{SW}}_{\mu \nu}(q) = \left[ \delta_{\mu \nu} \left( 2A \Phi + \sum_{\chi} J_{\mu \chi}(0) \right) - J_{\mu \nu}(q) \right] \langle S^z \rangle\] <p>where:</p> <ul> <li>\(J\_{\mu \nu}(q)\) are the Fourier transforms of the exchange interaction matrix.</li> <li>\(\Phi = 1 - \left( 1 - \langle S^2_z \rangle / 2 \right)\) is the Anderson-Callen decoupling factor for \(S = 1\).</li> </ul> <p>The magnon spectra exhibit optical and acoustic branches. Near the \(\Gamma\) point, the acoustic branch disperses quadratically:</p> \[\Omega_q \approx \Omega_0 + Dq^2\] <p>where \(D\) is the spin-stiffness constant and \(\Omega_0\) is the gap due to single-ion anisotropy.</p> <p><strong>Temperature-Dependent Magnetization</strong>:</p> <ul> <li>Magnetization \(\langle S^z \rangle\) is determined by spin-wave excitations, using the Tyablikov decoupling (RPA):</li> </ul> \[\langle (S^z_i)^n S^-_i S^+_i \rangle = \langle [S^+_i, (S^z_i)^n S^-_i] \rangle \sum_{q \nu} \langle b^{\dagger}_{q \nu} b_{q \nu} \rangle\] <p>where \(\langle b^{\dagger}_{q \nu} b_{q \nu} \rangle = [\exp(\Omega_{q \nu} / k_BT) - 1]^{-1}\) is the equilibrium magnon distribution.</p> <p>By solving these equations self-consistently, the renormalized exchange interactions \(J(T)\) are used to calculate the Curie temperature \(T_C\). The renormalized interactions lead to a reduced \(T_C\) compared to the non-renormalized case.</p> <p><strong>Context</strong>: The detailed calculation of spin-wave spectra and their temperature dependence provides insight into the stability of magnetic order and how it is influenced by electron-phonon interactions.</p> <h2 id="discussion">Discussion</h2> <p>The discussion highlights several key points:</p> <ol> <li> <p><strong>Adiabatic vs. Antiadiabatic Electron-Phonon Coupling</strong>:</p> <ul> <li>Most systems can be treated adiabatically, where phonon energies are much smaller than electron energies.</li> <li>For stronger renormalization effects, systems with narrow electron bands or high phonon energies relative to electron energies need to be considered.</li> </ul> </li> <li> <p><strong>Out-of-Equilibrium Effects</strong>:</p> <ul> <li>Non-equilibrium distributions, such as those induced by charge currents or laser fields, can enhance electron-phonon coupling.</li> <li>This can lead to significant changes in exchange interactions and magnetic properties.</li> </ul> </li> <li> <p><strong>Anisotropic Magnetic Interactions</strong>:</p> <ul> <li>The study focuses on isotropic exchange interactions, but anisotropic interactions, such as Dzyaloshinskii-Moriya interaction (DMI), can exhibit stronger renormalization effects.</li> </ul> </li> </ol> <p><strong>Conclusion</strong>: The study demonstrates that electron-phonon coupling significantly affects the magnetic properties of 2D metallic magnets by renormalizing the exchange interactions. This renormalization leads to a suppression of magnetic ordering temperatures and modifies the magnon spectra, which has implications for the design and application of 2D magnetic materials in technology.</p> <h3 id="one-or-two-d-line-mathematical-context">One (or two :D) line mathematical context</h3> <p><strong>Magnetic Force Theorem</strong>:</p> <ul> <li>The exchange interaction \(J\_{ij}\) is derived from the magnetic force theorem, which involves calculating the energy cost of rotating spins \(\mathbf{S}\_i\) and \(\mathbf{S}\_j\).</li> <li>The exchange interaction is given by the integral over the Brillouin zone of the product of the spin-resolved Green’s functions and exchange splitting.</li> </ul> <p><strong>Dyson Equation</strong>:</p> <ul> <li>The renormalization of Green’s functions due to self-energy \(\Sigma_k(i\omega_n)\) is given by the Dyson equation.</li> <li>This renormalizes both the propagators and the exchange splitting.</li> </ul> <p><strong>Electron-Phonon Self-Energy</strong>:</p> <ul> <li>The self-energy \(\Sigma^{\sigma}\_k(i\omega_n)\) represents the correction to the electron’s energy due to its interaction with phonons.</li> <li>The self-energy is calculated using second-order perturbation theory in the electron-phonon coupling.</li> </ul> <p><strong>Spin-Wave Theory</strong>:</p> <ul> <li>The spin-wave Hamiltonian is derived by expanding the Heisenberg Hamiltonian to second order in spin deviations.</li> <li>Diagonalizing the resulting Hamiltonian gives the magnon eigenvalues (frequencies) and eigenvectors.</li> </ul> <p><strong>Temperature-Dependent Magnetization</strong>:</p> <ul> <li>The magnetization \(\langle S^z \rangle\) is obtained using the Tyablikov decoupling method, which approximates the thermal averages of spin operators.</li> <li>The self-consistent solution of the magnetization equations provides the temperature dependence of \(\langle S^z \rangle\) and \(J(T)\).</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>I haven’t really thought of more implications, not further than what the authors imply.</p>]]></content><author><name></name></author><category term="journal-club"/><category term="physics"/><category term="magnetism"/><summary type="html"><![CDATA[A high-level summary of main ideas from Badrtdinov et al.]]></summary></entry><entry><title type="html">Cooperative Graph Neural Networks</title><link href="https://utksi.github.io/blog/2024/cognn/" rel="alternate" type="text/html" title="Cooperative Graph Neural Networks"/><published>2024-05-01T07:18:00+00:00</published><updated>2024-05-01T07:18:00+00:00</updated><id>https://utksi.github.io/blog/2024/cognn</id><content type="html" xml:base="https://utksi.github.io/blog/2024/cognn/"><![CDATA[<h2 id="detailed-analysis-of-cooperative-graph-neural-networks-co-gnns">Detailed Analysis of Cooperative Graph Neural Networks (Co-GNNs)</h2> <ul> <li>Proposed by <a href="https://arxiv.org/abs/2310.01267">Finkelstein et al.</a></li> </ul> <h3 id="framework-overview"><strong>Framework Overview</strong></h3> <p>Co-GNNs introduce a novel, flexible message-passing mechanism where each node in the graph dynamically selects from the actions: <code class="language-plaintext highlighter-rouge">listen</code>, <code class="language-plaintext highlighter-rouge">broadcast</code>, <code class="language-plaintext highlighter-rouge">listen and broadcast</code>, or <code class="language-plaintext highlighter-rouge">isolate</code>. This is facilitated by two cooperating networks:</p> <ol> <li><strong>Action Network (\pi)</strong>: Determines the optimal action for each node.</li> <li><strong>Environment Network (\eta)</strong>: Updates the node states based on the chosen actions.</li> </ol> <h2 id="mathematical-formulation"><strong>Mathematical Formulation</strong></h2> <ol> <li> <p><strong>Action Selection (Action Network (\pi))</strong>:</p> <ul> <li>For each node (v), the action network predicts a probability distribution (p^{(\ell)}_v) over the actions ({S, L, B, I}) at layer (\ell):</li> </ul> <p>[ p^{(\ell)}_v = \pi \left( h^{(\ell)}_v, { h^{(\ell)}_u \mid u \in N_v } \right) ]</p> <ul> <li>Actions are sampled using the Straight-through Gumbel-softmax estimator.</li> </ul> </li> <li> <p><strong>State Update (Environment Network (\eta))</strong>:</p> <ul> <li>The environment network updates the node states based on the selected actions:</li> </ul> <p>[ h^{(\ell+1)}_v = \begin{cases} \eta^{(\ell)} \left( h^{(\ell)}_v, { } \right) &amp; \text{if } a^{(\ell)}_v = \text{I or B} <br/> \eta^{(\ell)} \left( h^{(\ell)}_v, { h^{(\ell)}_u \mid u \in N_v, a^{(\ell)}_u = \text{S or B} } \right) &amp; \text{if } a^{(\ell)}_v = \text{L or S} \end{cases} ]</p> </li> <li> <p><strong>Layer-wise Update</strong>:</p> <ul> <li>A Co-GNN layer involves predicting actions, sampling them, and updating node states.</li> <li>Repeated for (L) layers to obtain final node representations (h^{(L)}_v).</li> </ul> </li> </ol> <h3 id="environment-network-eta-details"><strong>Environment Network (\eta) Details</strong></h3> <p>The environment network updates node states using a message-passing scheme based on the selected actions. Let’s consider the standard GCN layer and how it adapts to Co-GNN concepts:</p> <ol> <li> <p><strong>Message Aggregation</strong>:</p> <ul> <li>For each node (v), aggregate messages from its neighbors (u) that are broadcasting or using the standard action: [ m<em>v^{(\ell)} = \sum</em>{u \in N_v, a_u^{(\ell)} = \text{S or B}} h_u^{(\ell)} ]</li> </ul> </li> <li> <p><strong>Node Update</strong>:</p> <ul> <li>The node updates its state based on the aggregated messages and its current state: [ h_v^{(\ell+1)} = \sigma \left( W^{(\ell)}_s h_v^{(\ell)} + W^{(\ell)}_n m_v^{(\ell)} \right) ]</li> </ul> </li> </ol> <h3 id="properties-and-benefits"><strong>Properties and Benefits</strong></h3> <ul> <li><strong>Task-specific</strong>: Nodes learn to focus on relevant neighbors based on the task.</li> <li><strong>Directed</strong>: Edges can become directed, influencing directional information flow.</li> <li><strong>Dynamic and Feature-based</strong>: Adapt to changing graph structures and node features.</li> <li><strong>Asynchronous Updates</strong>: Nodes can be updated independently.</li> <li><strong>Expressive Power</strong>: More expressive than traditional GNNs, capable of handling long-range dependencies and reducing over-squashing and over-smoothing.</li> </ul> <h3 id="example-implementation"><strong>Example Implementation</strong></h3> <p>Consider a GCN (Graph Convolutional Network) adapted with Co-GNN concepts:</p> <ol> <li> <p><strong>GCN Layer (Traditional)</strong>:</p> <p>[ h^{(\ell+1)}<em>v = \sigma \left( W^{(\ell)}_s h^{(\ell)}_v + W^{(\ell)}_n \sum</em>{u \in N_v} h^{(\ell)}_u \right) ]</p> </li> <li> <p><strong>Co-GNN Layer</strong>:</p> <ul> <li><strong>Action Network</strong>: Predicts action probabilities for each node.</li> </ul> <p>[ p^{(\ell)}_v = \text{Softmax} \left( W_a h^{(\ell)}_v + b_a \right) ]</p> <ul> <li><strong>Action Sampling</strong>: Gumbel-softmax to select actions: [ a^{(\ell)}_v \sim \text{Gumbel-Softmax}(p^{(\ell)}_v) ]</li> <li><strong>State Update (Environment Network)</strong>:</li> </ul> <p>[ h^{(\ell+1)}<em>v = \begin{cases} \sigma \left( W^{(\ell)}_s h^{(\ell)}_v \right) &amp; \text{if } a^{(\ell)}_v = \text{I or B} <br/> \sigma \left( W^{(\ell)}_s h^{(\ell)}_v + W^{(\ell)}_n \sum</em>{u \in N_v, a^{(\ell)}_u = \text{S or B}} h^{(\ell)}_u \right) &amp; \text{if } a^{(\ell)}_v = \text{L or S} \end{cases} ]</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Co-GNNs represent a significant advancement in GNN architectures, offering a dynamic and adaptive message-passing framework that improves the handling of complex graph structures and long-range dependencies. The introduction of the Action Network and Environment Network provides a more flexible and task-specific approach to node state updates, leading to superior performance on various graph-related tasks.</p> <p>For further details, refer to the <a href="https://arxiv.org/abs/2310.01267">manuscript</a>.</p>]]></content><author><name></name></author><category term="worklog"/><category term="machine-learning-potential"/><category term="neural-network"/><summary type="html"><![CDATA[Some crucial ideas from Finklestein et al.'s work on Cooperative GNNs]]></summary></entry><entry><title type="html">E(3) - equivariant GNN with Learnable Activation Functions on Edges</title><link href="https://utksi.github.io/blog/2024/gnn_kan/" rel="alternate" type="text/html" title="E(3) - equivariant GNN with Learnable Activation Functions on Edges"/><published>2024-04-21T22:31:00+00:00</published><updated>2024-04-21T22:31:00+00:00</updated><id>https://utksi.github.io/blog/2024/gnn_kan</id><content type="html" xml:base="https://utksi.github.io/blog/2024/gnn_kan/"><![CDATA[<ul> <li>KANs proposed by <a href="https://arxiv.org/abs/2404.19756">Liu et al.</a>.</li> <li>See <a href="https://github.com/GistNoesis/FourierKAN">Fourier-KAN</a> implementation, replaces splines with Fourier coefficients.</li> </ul> <h2 id="general-message-passing-neural-network-mpnn">General Message Passing Neural Network (MPNN)</h2> <ol> <li> <p><strong>Input Node and Edge Features</strong>:</p> <ul> <li>Nodes: (\mathbf{x}_i) (node features)</li> <li>Edges: (\mathbf{e}_{ij}) (edge features)</li> </ul> </li> <li> <p><strong>Message Passing Layer</strong> (per layer):</p> <p>a. <strong>Edge Feature Transformation</strong>:</p> <p>[ \mathbf{e}’<em>{ij} = f_e(\mathbf{e}</em>{ij}) ]</p> <p>where (f_e) is a transformation function applied to edge features.</p> <p>b. <strong>Message Computation</strong>:</p> <p>[ \mathbf{m}<em>{ij} = f_m(\mathbf{x}_i, \mathbf{x}_j, \mathbf{e}’</em>{ij}) ]</p> <p>where (f<em>m) computes messages using node features (\mathbf{x}_i), (\mathbf{x}_j), and transformed edge features (\mathbf{e}’</em>{ij}).</p> <p>c. <strong>Message Aggregation</strong>:</p> <p>[ \mathbf{m}<em>i = \sum</em>{j \in \mathcal{N}(i)} \mathbf{m}_{ij} ]</p> <p>where (\mathcal{N}(i)) denotes the set of neighbors of node (i).</p> <p>d. <strong>Node Feature Update</strong>:</p> <p>[ \mathbf{x}’_i = f_n(\mathbf{x}_i, \mathbf{m}_i) ]</p> <p>where (f_n) updates node features using the aggregated messages (\mathbf{m}_i).</p> </li> <li> <p><strong>Output Node and Edge Features</strong>:</p> <ul> <li>Nodes: (\mathbf{x}’_i) (updated node features)</li> <li>Edges: (\mathbf{e}’_{ij}) (updated edge features)</li> </ul> </li> </ol> <h2 id="e3-equivariant-gnn-with-learnable-activation-functions-on-edges">E3-Equivariant GNN with Learnable Activation Functions on Edges</h2> <ol> <li> <p><strong>Input Node and Edge Features</strong>:</p> <ul> <li>Nodes: (\mathbf{x}_i) (node features)</li> <li>Edges: (\mathbf{e}_{ij}) (edge features)</li> </ul> </li> <li> <p><strong>Learnable Edge Feature Transformation</strong>:</p> <ul> <li> <p><strong>Fourier-based Edge Transformation</strong>:</p> <p>[ \mathbf{e}’<em>{ij} = \text{FourierTransform}(\mathbf{e}</em>{ij}) ]</p> <p>where the Fourier transformation is applied to edge features. Specifically, the transformation is defined as:</p> <p>[ \mathbf{e}’<em>{ij} = \sum</em>{k=1}^{K} a<em>{ij,k} \cos(k \mathbf{e}</em>{ij}) + b<em>{ij,k} \sin(k \mathbf{e}</em>{ij}) ]</p> <p>Here, (a<em>{ij,k}) and (b</em>{ij,k}) are learnable parameters, and (K) is the number of Fourier terms.</p> </li> </ul> </li> <li> <p><strong>Message Passing and Aggregation</strong>:</p> <p>a. <strong>Message Computation</strong>:</p> <p>[ \mathbf{m}<em>{ij} = \mathbf{e}’</em>{ij} \odot \mathbf{x}_j ]</p> <p>where (\odot) denotes element-wise multiplication, combining the transformed edge features (\mathbf{e}’_{ij}) with the neighboring node features (\mathbf{x}_j).</p> <p>b. <strong>Message Aggregation</strong>:</p> <p>[ \mathbf{m}<em>i = \sum</em>{j \in \mathcal{N}(i)} \mathbf{m}_{ij} ]</p> <p>c. <strong>Simple Node Feature Transformation</strong>:</p> <p>[ \mathbf{x}’_i = \mathbf{W} (\mathbf{x}_i + \mathbf{m}_i) + \mathbf{b} ]</p> <p>where (\mathbf{W}) is a learnable weight matrix and (\mathbf{b}) is a bias vector.</p> </li> <li> <p><strong>Output Node and Edge Features</strong>:</p> <ul> <li>Nodes: (\mathbf{x}’_i) (updated node features)</li> <li>Edges: (\mathbf{e}’_{ij}) (updated edge features)</li> </ul> </li> </ol> <h2 id="full-implementation">Full Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch_scatter</span> <span class="kn">import</span> <span class="n">scatter_add</span>
<span class="kn">from</span> <span class="n">torch_geometric.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">QM9</span>
<span class="kn">from</span> <span class="n">torch_geometric.transforms</span> <span class="kn">import</span> <span class="n">Distance</span>
<span class="kn">from</span> <span class="n">torch_geometric.nn</span> <span class="kn">import</span> <span class="n">MessagePassing</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="n">e3nn</span> <span class="kn">import</span> <span class="n">o3</span>
<span class="kn">from</span> <span class="n">e3nn.nn</span> <span class="kn">import</span> <span class="n">Gate</span><span class="p">,</span> <span class="n">FullyConnectedNet</span>


<span class="k">class</span> <span class="nc">LearnableActivationEdge</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Class to define learnable activation functions on edges using Fourier series.
    Inspired by Kolmogorov-Arnold Networks (KANs) to capture complex, non-linear transformations on edge features.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputdim</span><span class="p">,</span> <span class="n">outdim</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">,</span> <span class="n">addbias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize the LearnableActivationEdge module.

        Args:
            inputdim (int): Dimension of input edge features.
            outdim (int): Dimension of output edge features.
            num_terms (int): Number of Fourier terms.
            addbias (bool): Whether to add a bias term. Default is True.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LearnableActivationEdge</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">num_terms</span> <span class="o">=</span> <span class="n">num_terms</span>
        <span class="n">self</span><span class="p">.</span><span class="n">addbias</span> <span class="o">=</span> <span class="n">addbias</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inputdim</span> <span class="o">=</span> <span class="n">inputdim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outdim</span> <span class="o">=</span> <span class="n">outdim</span>

        <span class="c1"># Initialize learnable Fourier coefficients
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fouriercoeffs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">outdim</span><span class="p">,</span> <span class="n">inputdim</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">)</span> <span class="o">/</span>
            <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">inputdim</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">num_terms</span><span class="p">)))</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">addbias</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">outdim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass to apply learnable activation functions on edge attributes.

        Args:
            edge_attr (Tensor): Edge attributes of shape (..., inputdim).

        Returns:
            Tensor: Transformed edge attributes of shape (..., outdim).
        </span><span class="sh">"""</span>
        <span class="n">xshp</span> <span class="o">=</span> <span class="n">edge_attr</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">outshape</span> <span class="o">=</span> <span class="n">xshp</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">outdim</span><span class="p">,)</span>
        <span class="n">edge_attr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">edge_attr</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">inputdim</span><span class="p">))</span>

        <span class="c1"># Generate Fourier terms
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_terms</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">edge_attr</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_terms</span><span class="p">)</span>
        <span class="n">xrshp</span> <span class="o">=</span> <span class="n">edge_attr</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute cosine and sine components
</span>        <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">xrshp</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">xrshp</span><span class="p">)</span>

        <span class="c1"># Apply learnable Fourier coefficients
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">fouriercoeffs</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">fouriercoeffs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Add bias if applicable
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">addbias</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>

        <span class="c1"># Reshape to original edge attribute shape
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">outshape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span> <span class="nc">E3EquivariantGNN</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    E(3)-Equivariant Graph Neural Network (GNN) that focuses on learnable activation functions on edges.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize the E3EquivariantGNN module.

        Args:
            in_features (int): Dimension of input node features.
            out_features (int): Dimension of output node features.
            hidden_dim (int): Dimension of hidden layers.
            num_layers (int): Number of layers in the network.
            num_terms (int): Number of Fourier terms for learnable activation functions.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">E3EquivariantGNN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="sh">'</span><span class="s">add</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

        <span class="c1"># Define the input and output irreps (representations)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_irrep</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="n">Irreps</span><span class="p">.</span><span class="nf">spherical_harmonics</span><span class="p">(</span><span class="n">lmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Example irreps, adjust as needed
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_irrep</span> <span class="o">=</span> <span class="n">o3</span><span class="p">.</span><span class="nc">Irreps</span><span class="p">([(</span><span class="n">out_features</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))])</span>  <span class="c1"># Scalar output
</span>
        <span class="c1"># Define the hidden irreps
</span>        <span class="n">hidden_irreps</span> <span class="o">=</span> <span class="p">[</span><span class="n">o3</span><span class="p">.</span><span class="n">Irreps</span><span class="p">.</span><span class="nf">spherical_harmonics</span><span class="p">(</span><span class="n">lmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>  <span class="c1"># Adjust as needed
</span>
        <span class="c1"># Create the equivariant layers and learnable activation functions on edges
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fourier_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">LearnableActivationEdge</span><span class="p">(</span><span class="n">in_features</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_terms</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">Gate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_irrep</span><span class="p">,</span> <span class="n">hidden_irreps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">num_terms</span><span class="p">),</span>
            <span class="o">*</span><span class="p">[</span><span class="nc">Gate</span><span class="p">(</span><span class="n">hidden_irreps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hidden_irreps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">num_terms</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)],</span>
            <span class="nc">Gate</span><span class="p">(</span><span class="n">hidden_irreps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">output_irrep</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">num_terms</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># Output layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_attr</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass to propagate node features through the GNN.

        Args:
            x (Tensor): Node features of shape (num_nodes, in_features).
            edge_index (Tensor): Edge indices of shape (2, num_edges).
            edge_attr (Tensor): Edge attributes of shape (num_edges, edge_dim).

        Returns:
            Tensor: Output node features of shape (num_nodes, out_features).
        </span><span class="sh">"""</span>
        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">edge_index</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="c1"># Transform edge features with Fourier series
</span>            <span class="n">fourier_messages</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">fourier_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">edge_attr</span><span class="p">)</span>

            <span class="c1"># Apply equivariant transformations to node features
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">fourier_messages</span><span class="p">)</span>

            <span class="c1"># Compute messages
</span>            <span class="n">m_ij</span> <span class="o">=</span> <span class="n">fourier_messages</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">row</span><span class="p">]</span>

            <span class="c1"># Aggregate messages
</span>            <span class="n">m_i</span> <span class="o">=</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">m_ij</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim_size</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

            <span class="c1"># Update node features
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">m_i</span>

        <span class="c1"># Apply the final linear layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="c1"># Load and prepare the QM9 dataset
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nc">QM9</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/QM9</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="nc">Distance</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Split dataset into training, validation, and test sets
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:</span><span class="mi">110000</span><span class="p">]</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">110000</span><span class="p">:</span><span class="mi">120000</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">120000</span><span class="p">:]</span>

<span class="c1"># Data loaders for training, validation, and test sets
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Define the loss function and optimizer
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">E3EquivariantGNN</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_terms</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Perform a single training step.

    Args:
        model (nn.Module): The neural network model.
        optimizer (Optimizer): The optimizer.
        criterion (Loss): The loss function.
        data (Data): The input data batch.

    Returns:
        float: The loss value.
    </span><span class="sh">"""</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_attr</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>


<span class="c1"># Training loop
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

    <span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_attr</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">)</span>
            <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">val_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_loader</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Val Loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h2 id="detailed-explanation-of-mathematical-formulations">Detailed Explanation of Mathematical Formulations</h2> <h3 id="learnable-edge-feature-transformation">Learnable Edge Feature Transformation</h3> <p>For each edge $(i, j)$ with feature $\mathbf{e}_{ij}$:</p> \[\mathbf{e}'_{ij} = \sum_{k=1}^{K} a_{ij,k} \cos(k \mathbf{e}_{ij}) + b_{ij,k} \sin(k \mathbf{e}_{ij})\] <p>where $a_{ij,k}$ and $b_{ij,k}$ are learnable parameters, and $K$ is the number of terms.</p> <h3 id="message-computation">Message Computation</h3> <p>For each edge $(i, j)$:</p> \[\mathbf{m}_{ij} = \mathbf{e}'_{ij} \odot \mathbf{x}_j\] <p>where $\odot$ denotes element-wise multiplication.</p> <h3 id="message-aggregation">Message Aggregation</h3> <p>For each node $i$:</p> \[\mathbf{m}_i = \sum_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}\] <p>where $\mathcal{N}(i)$ denotes the set of neighbors of node $i$.</p> <h3 id="node-feature-update">Node Feature Update</h3> <p>For each node $i$:</p> \[\mathbf{x}'_i = \mathbf{W} (\mathbf{x}_i + \mathbf{m}_i) + \mathbf{b}\] <p>where $\mathbf{W}$ is a learnable weight matrix and $\mathbf{b}$ is a bias vector.</p> <h2 id="summary">Summary</h2> <p>This implementation combines the learnable activation functions on edges with E(3) equivariant transformations on node features. The detailed mathematical formulations provided in the comments explain each step of the process, making it suitable for a physicist audience familiar with these concepts.</p> <p>#Idea #TODO: KANs for learnable edge activations in MACE - to have it as an option. Train on the same set.</p>]]></content><author><name></name></author><category term="worklog"/><category term="machine-learning"/><category term="neural-network"/><summary type="html"><![CDATA[Some ideas about KAN-based GNNs beyond just stacking layers]]></summary></entry><entry><title type="html">[Stuff I read] Kolmogorov Arnold Networks</title><link href="https://utksi.github.io/blog/2024/kan/" rel="alternate" type="text/html" title="[Stuff I read] Kolmogorov Arnold Networks"/><published>2024-04-18T22:31:00+00:00</published><updated>2024-04-18T22:31:00+00:00</updated><id>https://utksi.github.io/blog/2024/kan</id><content type="html" xml:base="https://utksi.github.io/blog/2024/kan/"><![CDATA[<ul> <li>Proposed by Max Tegmark’s group. See <a href="https://arxiv.org/abs/2404.19756">Liu et al.</a></li> <li>A hot topic on Twitter - a matter of a lot of debate.</li> </ul> <p>The paper “KAN: Kolmogorov–Arnold Networks” proposes Kolmogorov-Arnold Networks (KANs) as an alternative to Multi-Layer Perceptrons (MLPs). The core idea behind KANs is inspired by the Kolmogorov-Arnold representation theorem, which states that any multivariate continuous function can be represented as a sum of continuous functions of one variable. This section will summarize the technical details of the paper, focusing on the mathematical formulations.</p> <h3 id="1-introduction">1. Introduction</h3> <p>The motivation for KANs stems from the limitations of MLPs, such as fixed activation functions on nodes and linear weights. MLPs rely heavily on the universal approximation theorem, but their structure can be less efficient and interpretable. KANs, on the other hand, utilize learnable activation functions on edges and replace linear weights with univariate functions parametrized as splines.</p> <h3 id="2-kolmogorov-arnold-representation-theorem">2. Kolmogorov-Arnold Representation Theorem</h3> <p>The Kolmogorov-Arnold representation theorem states:</p> <p>[ f(x) = \sum<em>{q=1}^{2n+1} \Phi_q \left( \sum</em>{p=1}^n \varphi_{q,p}(x_p) \right) ]</p> <p>where (\varphi_{q,p} : [0, 1] \to \mathbb{R}) and (\Phi_q : \mathbb{R} \to \mathbb{R}).</p> <h3 id="3-kan-architecture">3. KAN Architecture</h3> <p>KANs generalize the representation theorem to arbitrary depths and widths. Each weight parameter in KANs is replaced by a learnable 1D function (spline).</p> <h4 id="31-mathematical-formulation-of-kans">3.1. Mathematical Formulation of KANs</h4> <p>Define a KAN layer with (n<em>{\text{in}})-dimensional inputs and (n</em>{\text{out}})-dimensional outputs as a matrix of 1D functions:</p> <p>[ \Phi = { \varphi<em>{q,p} }, \quad p = 1, 2, \ldots, n</em>{\text{in}}, \quad q = 1, 2, \ldots, n_{\text{out}} ]</p> <p>Activation function on edge (\varphi_{l,j,i}) between layer (l) and (l+1) is given by:</p> <p>[ \varphi_{l,j,i}(x) = w \big(b(x) + \text{spline}(x)\big) ]</p> <p>where (b(x) = \text{silu}(x) = \frac{x}{1 + e^{-x}}).</p> <p>The output of each layer is computed as:</p> <p>[ x<em>{l+1, j} = \sum</em>{i=1}^{n<em>l} \varphi</em>{l,j,i}(x_{l,i}) ]</p> <p>in matrix form:</p> <p>[ x_{l+1} = \Phi_l x_l ]</p> <p>where (\Phi_l) is the function matrix of layer (l).</p> <h3 id="4-approximation-abilities-and-scaling-laws">4. Approximation Abilities and Scaling Laws</h3> <p>KANs can approximate functions by decomposing high-dimensional problems into several 1D problems, effectively avoiding the curse of dimensionality.</p> <h4 id="theorem-21-approximation-bound">Theorem 2.1: Approximation Bound</h4> <p>Let (f(x)) be represented as:</p> <p>[ f = (\Phi<em>{L-1} \circ \Phi</em>{L-2} \circ \cdots \circ \Phi_1 \circ \Phi_0)x ]</p> <p>For each (\Phi<em>{l,i,j}), there exist (k)-th order B-spline functions (\Phi</em>{l,i,j}^G) such that:</p> <p>[ | f - (\Phi<em>{L-1}^G \circ \Phi</em>{L-2}^G \circ \cdots \circ \Phi<em>1^G \circ \Phi_0^G)x |</em>{C^m} \leq C G^{-k-1+m} ]</p> <p>where (G) is the grid size and (C) depends on (f) and its representation.</p> <h3 id="5-grid-extension-technique">5. Grid Extension Technique</h3> <p>KANs can increase accuracy by refining the grid used in splines:</p> <p>[ {c’<em>j} = \arg\min</em>{{c’<em>j}} E</em>{x \sim p(x)} \left( \sum<em>{j=0}^{G2+k-1} c’_j B’_j(x) - \sum</em>{i=0}^{G1+k-1} c_i B_i(x) \right)^2 ]</p> <h3 id="6-simplification-techniques">6. Simplification Techniques</h3> <p>KANs can be made more interpretable by sparsification, pruning, and symbolification. The (L^1) norm and entropy regularization can be used to sparsify the network.</p> <h3 id="7-toy-examples-and-empirical-results">7. Toy Examples and Empirical Results</h3> <p>KANs were shown to have better scaling laws than MLPs, achieving lower test losses with fewer parameters in various toy datasets and special functions.</p> <h4 id="example-functions">Example Functions:</h4> <ol> <li>Bessel function: (f(x) = J_0(20x))</li> <li>High-dimensional function:</li> </ol> <p>[ f(x<em>1, \ldots, x</em>{100}) = \exp\left( \frac{1}{100} \sum_{i=1}^{100} \sin^2\left(\frac{\pi x_i}{2}\right) \right) ]</p> <p>KANs can achieve near-theoretical scaling exponents (\alpha = 4), outperforming MLPs in accuracy and parameter efficiency.</p> <h3 id="conclusion">Conclusion</h3> <p>KANs provide a novel approach to neural network design, leveraging the Kolmogorov-Arnold representation theorem to achieve better performance and interpretability compared to traditional MLPs. The use of learnable activation functions on edges and splines allows for greater flexibility and efficiency in function approximation.</p>]]></content><author><name></name></author><category term="journal-club"/><category term="neural-network"/><summary type="html"><![CDATA[A very short summary of main ideas from Liu et al.]]></summary></entry><entry><title type="html">MACE (Message Passing ACE)</title><link href="https://utksi.github.io/blog/2023/mace/" rel="alternate" type="text/html" title="MACE (Message Passing ACE)"/><published>2023-08-01T22:36:00+00:00</published><updated>2023-08-01T22:36:00+00:00</updated><id>https://utksi.github.io/blog/2023/mace</id><content type="html" xml:base="https://utksi.github.io/blog/2023/mace/"><![CDATA[<h3 id="introduction"><strong>Introduction</strong></h3> <p>MACE (Message Passing Atomic Cluster Expansion) is an equivariant message-passing neural network that uses higher-order messages to enhance the accuracy and efficiency of force fields in computational chemistry.</p> <h3 id="node-representation"><strong>Node Representation</strong></h3> <p>Each node (\large{i}) is represented by:</p> <p>[ \large{\sigma_i^{(t)} = (r_i, z_i, h_i^{(t)})} ]</p> <p>where (r_i \in \mathbb{R}^3) is the position, (\large{z_i}) is the chemical element, and (\large{h_i^{(t)}}) are the learnable features at layer (\large{t}).</p> <h3 id="message-construction"><strong>Message Construction</strong></h3> <p>Messages are constructed hierarchically using a body order expansion:</p> <p>[ m<em>i^{(t)} = \sum_j u_1(\sigma_i^{(t)}, \sigma_j^{(t)}) + \sum</em>{j<em>1, j_2} u_2(\sigma_i^{(t)}, \sigma</em>{j<em>1}^{(t)}, \sigma</em>{j<em>2}^{(t)}) + \cdots + \sum</em>{j<em>1, \ldots, j</em>\nu} u<em>\nu(\sigma_i^{(t)}, \sigma</em>{j<em>1}^{(t)}, \ldots, \sigma</em>{j_\nu}^{(t)}) ]</p> <h3 id="two-body-message-construction"><strong>Two-body Message Construction</strong></h3> <p>For two-body interactions, the message (m_i^{(t)}) is:</p> <p>[ A<em>i^{(t)} = \sum</em>{j \in N(i)} R<em>{kl_1l_2l_3}^{(t)}(r</em>{ij}) Y<em>{l_1}^{m_1}(\hat{r}</em>{ij}) W<em>{kk_2l_2}^{(t)} h</em>{j,k_2l_2m_2}^{(t)} ]</p> <p>where (\large{R}) is a learnable radial basis, (\large{Y}) are spherical harmonics, and (\large{W}) are learnable weights. (\large{C}) are Clebsch-Gordan coefficients ensuring equivariance.</p> <h3 id="higher-order-feature-construction"><strong>Higher-order Feature Construction</strong></h3> <p>Higher-order features are constructed using tensor products and symmetrization:</p> <p>[ \large{B<em>{i, \eta \nu k LM}^{(t)} = \sum</em>{lm} C<em>{LM \eta \nu, lm} \prod</em>{\xi=1}^\nu \sum<em>{k</em>\xi} w<em>{kk</em>\xi l<em>\xi}^{(t)} A</em>{i, k<em>\xi l</em>\xi m_\xi}^{(t)}} ]</p> <p>where (\large{C}) are generalized Clebsch-Gordan coefficients.</p> <h3 id="message-passing"><strong>Message Passing</strong></h3> <p>The message passing updates the node features by aggregating messages:</p> <p>[ \large{h<em>i^{(t+1)} = U</em>{kL}^{(t)}(\sigma<em>i^{(t)}, m_i^{(t)}) = \sum</em>{k’} W<em>{kL, k’}^{(t)} m</em>{i, k’ LM} + \sum<em>{k’} W</em>{z<em>i kL, k’}^{(t)} h</em>{i, k’ LM}^{(t)}} ]</p> <h3 id="readout-phase"><strong>Readout Phase</strong></h3> <p>In the readout phase, invariant features are mapped to site energies:</p> <p>[ \large{E_i = E_i^{(0)} + E_i^{(1)} + \cdots + E_i^{(T)}} ]</p> <p>where:</p> <p>[ \large{E<em>i^{(t)} = R_t(h_i^{(t)}) = \sum</em>{k’} W<em>{\text{readout}, k’}^{(t)} h</em>{i, k’ 00}^{(t)} \quad \text{for } t &lt; T} ]</p> <p>[ \large{E<em>i^{(T)} = \text{MLP}</em>{\text{readout}}^{(t)}({h_{i, k 00}^{(t)}})} ]</p> <h3 id="equivariance"><strong>Equivariance</strong></h3> <p>The model ensures equivariance under rotation (\large{Q \in O(3)}):</p> <p>[ \large{h_i^{(t)}(Q \cdot (r_1, \ldots, r_N)) = D(Q) h_i^{(t)}(r_1, \ldots, r_N)} ]</p> <p>where (\large{D(Q)}) is a Wigner D-matrix. For feature (\large{h_{i, k LM}^{(t)}}), it transforms as:</p> <p>[ \large{h<em>{i, k LM}^{(t)}(Q \cdot (r_1, \ldots, r_N)) = \sum</em>{M’} D<em>L(Q)</em>{M’M} h_{i, k LM’}^{(t)}(r_1, \ldots, r_N)} ]</p> <h2 id="properties-and-computational-efficiency">Properties and Computational Efficiency</h2> <ol> <li> <p><strong>Body Order Expansion</strong>:</p> <ul> <li>MACE constructs messages using higher body order expansions, enabling rich representations of atomic environments.</li> </ul> </li> <li> <p><strong>Computational Efficiency</strong>:</p> <ul> <li>The use of higher-order messages reduces the required number of message-passing layers to two, enhancing computational efficiency and scalability.</li> </ul> </li> <li> <p><strong>Receptive Field</strong>:</p> <ul> <li>MACE maintains a small receptive field by decoupling correlation order increase from the number of message-passing iterations, facilitating parallelization.</li> </ul> </li> <li> <p><strong>State-of-the-Art Performance</strong>:</p> <ul> <li>MACE achieves state-of-the-art accuracy on benchmark tasks (rMD17, 3BPA, AcAc), demonstrating its effectiveness in modeling complex atomic interactions.</li> </ul> </li> </ol> <p>For further details, refer to the <a href="https://arxiv.org/abs/2206.07697">Batatia et al.</a>.</p> <hr/> <h2 id="necessary-math-to-know">Necessary Math to Know</h2> <h3 id="1-spherical-harmonics">1. <strong>Spherical Harmonics</strong></h3> <p><strong>Concept</strong>:</p> <ul> <li>Spherical harmonics (Y^L_M) are functions defined on the surface of a sphere. They are used in many areas of physics, including quantum mechanics and electrodynamics, to describe the angular part of a system.</li> </ul> <p><strong>Role in MACE</strong>:</p> <ul> <li>Spherical harmonics are used to decompose the angular dependency of the atomic environment. This helps in capturing the rotational properties of the features in a systematic way.</li> </ul> <p><strong>Mathematically</strong>:</p> <ul> <li>The spherical harmonics (Y^L_M(\theta, \phi)) are given by:</li> </ul> <p>[ Y^L_M(\theta, \phi) = \sqrt{\frac{(2L+1)}{4\pi} \frac{(L-M)!}{(L+M)!}} P^M_L(\cos \theta) e^{iM\phi} ]</p> <p>where (P^M_L) are the associated Legendre polynomials.</p> <h3 id="2-clebsch-gordan-coefficients">2. <strong>Clebsch-Gordan Coefficients</strong></h3> <p><strong>Concept</strong>:</p> <ul> <li>Clebsch-Gordan coefficients are used in quantum mechanics to combine angular momenta. They arise in the coupling of two angular momentum states to form a new angular momentum state.</li> </ul> <p><strong>Role in MACE</strong>:</p> <ul> <li>In MACE, Clebsch-Gordan coefficients are used to combine features from different atoms while maintaining rotational invariance. They ensure that the resulting features transform correctly under rotations, preserving the physical symmetry of the system.</li> </ul> <p><strong>Mathematically</strong>:</p> <ul> <li>When combining two angular momentum states (\vert l_1, m_1\rangle) and (\vert l_2, m_2\rangle), the resulting state (\vert L, M\rangle) is given by:</li> </ul> <p>[ |L, M\rangle = \sum<em>{m_1, m_2} C</em>{L, M}^{l_1, m_1; l_2, m_2} |l_1, m_1\rangle |l_2, m_2\rangle ]</p> <p>where (C_{L, M}^{l_1, m_1; l_2, m_2}) are the Clebsch-Gordan coefficients.</p> <h3 id="3-o3-rotations">3. <strong>(O(3)) Rotations</strong></h3> <p><strong>Concept</strong>:</p> <ul> <li>The group (O(3)) consists of all rotations and reflections in three-dimensional space. It represents the symmetries of a 3D system, including operations that preserve the distance between points.</li> </ul> <p><strong>Role in MACE</strong>:</p> <ul> <li>Ensuring that the neural network respects (O(3)) symmetry is crucial for modeling physical systems accurately. MACE achieves this by using operations that are invariant or equivariant under these rotations and reflections.</li> </ul> <p><strong>Mathematically</strong>:</p> <ul> <li>A rotation in (O(3)) can be represented by a 3x3 orthogonal matrix (Q) such that:</li> </ul> <p>[ Q^T Q = I \quad \text{and} \quad \det(Q) = \pm 1 ]</p> <p>where (I) is the identity matrix.</p> <h3 id="4-wigner-d-matrix">4. <strong>Wigner D-matrix</strong></h3> <p><strong>Concept</strong>:</p> <ul> <li>The Wigner D-matrix (D^L(Q)) represents the action of a rotation (Q) on spherical harmonics. It provides a way to transform the components of a tensor under rotation.</li> </ul> <p><strong>Role in MACE</strong>:</p> <ul> <li>Wigner D-matrices are used to ensure that the feature vectors in the neural network transform correctly under rotations. This is essential for maintaining the rotational equivariance of the model.</li> </ul> <p><strong>Mathematically</strong>:</p> <ul> <li>For a rotation (Q \in O(3)) and a spherical harmonic of degree (L), the Wigner D-matrix (D^L(Q)) is a ((2L+1) \times (2L+1)) matrix. If (Y^L_M) is a spherical harmonic, then under rotation (Q), it transforms as:</li> </ul> <p>[ Y^L<em>M(Q \cdot \mathbf{r}) = \sum</em>{M’=-L}^{L} D^L<em>{M’M}(Q) Y^L</em>{M’}(\mathbf{r}) ]</p>]]></content><author><name></name></author><category term="worklog"/><category term="machine"/><category term="learning"/><category term="potential"/><summary type="html"><![CDATA[A summary of Message Passing Atomic Cluster Expansion Graph Neural Networks]]></summary></entry></feed>